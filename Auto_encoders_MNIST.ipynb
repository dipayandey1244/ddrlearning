{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Libraries"
      ],
      "metadata": {
        "id": "1-__wFm5HudX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image"
      ],
      "metadata": {
        "id": "Sf3KOBOLHuOg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Defining Hyper parameters"
      ],
      "metadata": {
        "id": "vK6p7fSwH2_W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperparameters** in Variable Autoencoders: Hyperparameters are configurable variables set prior to training a Variable Autoencoder model. Their values influence the overall architecture and functioning of the model.\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "7eNr9WClSS6u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Role of Hyperparameters**: Setting appropriate hyperparameter values is crucial because they affect the model’s ability to learn and generalize effectively. For instance, choosing the right number of hidden layers, neurons per layer, encoding dimensions, and decoding dimensions can impact the model’s capacity to represent complex patterns in the data. Additionally, selecting suitable learning rates, batch sizes, activation functions, and optimization algorithms contribute to efficient training and optimal convergence.\n",
        "\n"
      ],
      "metadata": {
        "id": "pK8sRx6yS-Xc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Impact of Hyperparameters:** Effective tuning of hyperparameters leads to improved performance and better representation learning abilities in Variable Autoencoder models. On the contrary, poorly chosen hyperparameters might lead to underfitting, overfitting, slow convergence, or even failure to train the model. Therefore, understanding their role and proper selection is essential for achieving satisfactory results when working with Variable Autoencoders."
      ],
      "metadata": {
        "id": "f-rdHepsTIji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters\n",
        "image_size = 784\n",
        "hidden_dim = 400\n",
        "latent_dim = 20\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# MNIST dataset\n",
        "train_dataset = torchvision.datasets.MNIST(root='../../data',\n",
        "                                           train=True,\n",
        "                                           transform=transforms.ToTensor(),\n",
        "                                           download=True)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='../../data',\n",
        "                                          train=False,\n",
        "                                          transform=transforms.ToTensor())\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True)\n",
        "\n",
        "# Create directory to save the reconstructed and sampled images (if directory not present)\n",
        "sample_dir = 'results'\n",
        "if not os.path.exists(sample_dir):\n",
        "    os.makedirs(sample_dir)"
      ],
      "metadata": {
        "id": "YTJBbb-PH0uX",
        "outputId": "ac11fff2-e5fc-4f7e-e523-851ae29d5a8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ../../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 16397628.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../../data/MNIST/raw/train-images-idx3-ubyte.gz to ../../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 513868.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 4469853.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 5190879.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../../data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variable Auto Encoder Model\n",
        "\n",
        "**Autoencoder models** are a type of neural network designed primarily for unsupervised learning. They consist of two main components: an encoder and a decoder. The primary goal of an Autoencoder is to learn efficient representations (compress data) by reconstructing the original inputs from the encoded versions.\n",
        "\n",
        "The encoder maps the input data into a lower-dimensional latent space called **the bottleneck or encoding layer**. It compresses the information present in the input data while retaining important features. The decoder then takes the compressed representation from the encoder and attempts to reconstruct the original input.\n",
        "\n",
        "Autoencoders have several applications, including **dimensionality reduction, denoising data, feature extraction, anomaly detection, and generative modeling**.\n",
        "\n",
        "In dimensionality reduction, the encoder learns a compact representation of the input data, reducing the number of features without losing significant information. Denoising involves removing noise from corrupted data by learning clean representations. Feature extraction refers to extracting meaningful features from raw data, allowing easier interpretation and further use in supervised learning tasks. Anomaly detection identifies unusual data points based on differences between the original input and the reconstruction provided by the Autoencoder. Lastly, generative modeling allows creating new samples similar to existing ones by feeding random noise to the encoder and letting the decoder generate outputs.\n",
        "\n",
        "Variable Autoencoders extend traditional Autoencoders by introducing variability in the latent space, enabling more flexible representation learning. By adding stochastic elements to the encoder and decoder networks, Variable Autoencoders allow capturing multiple modes within the data distribution. This property makes them particularly useful for handling nonlinear relationships and complex distributions found in real-world data."
      ],
      "metadata": {
        "id": "SCTZX2mqRycR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Autoencoders are a powerful type of neural network architecture used by big tech companies for various applications. Here are some key areas where you'll find them being used, along with real-world examples:\n",
        "\n",
        "1. Anomaly Detection:\n",
        "\n",
        "Concept: Autoencoders are trained to reconstruct their input data. When they encounter a data point with significant reconstruction error (i.e., the recreated version deviates heavily from the original), it's likely an anomaly.\n",
        "Real-world Example: Netflix: Netflix uses autoencoders to detect anomalies in user viewing patterns. If a user's viewing habits suddenly deviate from their usual preferences, it could indicate fraudulent account access. This can help Netflix identify and address potential security issues.\n",
        "2. Data Compression:\n",
        "\n",
        "Concept: Autoencoders can learn a compressed representation of the input data while retaining essential features. This compressed data can be used for storage optimization or faster transmission.\n",
        "Real-world Example: Facebook: Facebook utilizes autoencoders to compress images and videos uploaded to their platform. This allows them to store and transmit this data more efficiently, reducing infrastructure costs and improving user experience by enabling faster loading times.\n",
        "3. Feature Extraction:\n",
        "\n",
        "Concept: The bottleneck layer of an autoencoder, where the compressed representation resides, often captures the most critical features of the input data. This makes it valuable for feature extraction tasks used in various machine learning models.\n",
        "Real-world Example: Amazon: Amazon might use autoencoders to extract key features from product images on their platform. These features can then be fed into recommendation algorithms to suggest relevant products to customers based on their past purchases or browsing behavior.\n",
        "4. Image Denoising:\n",
        "\n",
        "Concept: Autoencoders can be trained to remove noise from images while preserving the underlying structure. This is achieved by learning a mapping from noisy images to their clean counterparts.\n",
        "Real-world Example: Google Photos: Google Photos potentially leverages autoencoders to improve the quality of user-uploaded images. By removing noise caused by compression or camera limitations, autoencoders can enhance the visual clarity of photos stored on Google's platform.\n",
        "5. Generative Tasks:\n",
        "\n",
        "Concept: Variational Autoencoders (VAEs) are a type of autoencoder that can be used to generate new data similar to the training data. This has applications in areas like image generation or product recommendation.\n",
        "Real-world Example: Pinterest: Pinterest could potentially use VAEs to generate new image recommendations for users based on their browsing history and saved pins. This allows Pinterest to personalize the user experience and keep users engaged by suggesting visually interesting content.\n",
        "These are just a few examples, and big tech companies are constantly exploring new applications for autoencoders. As machine learning models become more complex and data volumes continue to grow, autoencoders are expected to play an increasingly important role in various data processing tasks.\n",
        "\n"
      ],
      "metadata": {
        "id": "empl7h1oVK2p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(image_size, hidden_dim)\n",
        "        self.fc2_mean = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.fc2_logvar = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, image_size)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = F.relu(self.fc1(x))\n",
        "        mu = self.fc2_mean(h)\n",
        "        log_var = self.fc2_logvar(h)\n",
        "        return mu, log_var\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(logvar/2)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h = F.relu(self.fc3(z))\n",
        "        out = torch.sigmoid(self.fc4(h))\n",
        "        return out\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, 1, 28,28) --> (batch_size, 784)\n",
        "        mu, logvar = self.encode(x.view(-1, image_size))\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        reconstructed = self.decode(z)\n",
        "        return reconstructed, mu, logvar\n",
        "\n",
        "# Define model and optimizer\n",
        "model = VAE().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
      ],
      "metadata": {
        "id": "7a8FHbI7H9Po"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Loss\n",
        "def loss_function(reconstructed_image, original_image, mu, logvar):\n",
        "    # Binary Cross Entropy\n",
        "    bce = F.binary_cross_entropy(reconstructed_image, original_image.view(-1, 784), reduction = 'sum')\n",
        "    # kld = torch.sum(0.5 * torch.sum(logvar.exp() + mu.pow(2) - 1 - logvar, 1))\n",
        "    # K L Divergence term\n",
        "    kld = 0.5 * torch.sum(logvar.exp() + mu.pow(2) - 1 - logvar)\n",
        "    return bce + kld\n"
      ],
      "metadata": {
        "id": "gqqYHcHUIGDK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train function\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for i, (images, _) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        reconstructed, mu, logvar = model(images)\n",
        "        loss = loss_function(reconstructed, images, mu, logvar)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(\"Train Epoch {} [Batch {}/{}]\\tLoss: {:.3f}\".format(epoch, i, len(train_loader), loss.item()/len(images)))\n",
        "\n",
        "    print('=====> Epoch {}, Average Loss: {:.3f}'.format(epoch, train_loss/len(train_loader.dataset)))\n",
        "\n"
      ],
      "metadata": {
        "id": "mamz9d5-IQ32"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Test function\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (images, _) in enumerate(test_loader):\n",
        "            images = images.to(device)\n",
        "            reconstructed, mu, logvar = model(images)\n",
        "            test_loss += loss_function(reconstructed, images, mu, logvar).item()\n",
        "            if batch_idx == 0:\n",
        "                comparison = torch.cat([images[:5], reconstructed.view(batch_size, 1, 28, 28)[:5]])\n",
        "                save_image(comparison.cpu(), 'results/reconstruction_' + str(epoch) + '.png', nrow = 5)\n",
        "\n",
        "    print('=====> Average Test Loss: {:.3f}'.format(test_loss/len(test_loader.dataset)))"
      ],
      "metadata": {
        "id": "SULfg33rITlW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Main function\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(epoch)\n",
        "    test(epoch)\n",
        "    with torch.no_grad():\n",
        "        # Get rid of the encoder and sample z from the gaussian ditribution and feed it to the decoder to generate samples\n",
        "        sample = torch.randn(64,20).to(device)\n",
        "        generated = model.decode(sample).cpu()\n",
        "        save_image(generated.view(64,1,28,28), 'results/sample_' + str(epoch) + '.png')"
      ],
      "metadata": {
        "id": "4GIKjRaTIYei",
        "outputId": "722edd35-e332-4615-e1a1-336805611975",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch 1 [Batch 0/469]\tLoss: 547.280\n",
            "Train Epoch 1 [Batch 100/469]\tLoss: 183.040\n",
            "Train Epoch 1 [Batch 200/469]\tLoss: 151.355\n",
            "Train Epoch 1 [Batch 300/469]\tLoss: 137.569\n",
            "Train Epoch 1 [Batch 400/469]\tLoss: 134.028\n",
            "=====> Epoch 1, Average Loss: 163.875\n",
            "=====> Average Test Loss: 128.640\n",
            "Train Epoch 2 [Batch 0/469]\tLoss: 128.948\n",
            "Train Epoch 2 [Batch 100/469]\tLoss: 120.071\n",
            "Train Epoch 2 [Batch 200/469]\tLoss: 125.021\n",
            "Train Epoch 2 [Batch 300/469]\tLoss: 120.767\n",
            "Train Epoch 2 [Batch 400/469]\tLoss: 121.096\n",
            "=====> Epoch 2, Average Loss: 122.219\n",
            "=====> Average Test Loss: 116.592\n",
            "Train Epoch 3 [Batch 0/469]\tLoss: 119.334\n",
            "Train Epoch 3 [Batch 100/469]\tLoss: 115.349\n",
            "Train Epoch 3 [Batch 200/469]\tLoss: 115.099\n",
            "Train Epoch 3 [Batch 300/469]\tLoss: 113.851\n",
            "Train Epoch 3 [Batch 400/469]\tLoss: 112.155\n",
            "=====> Epoch 3, Average Loss: 114.947\n",
            "=====> Average Test Loss: 112.369\n",
            "Train Epoch 4 [Batch 0/469]\tLoss: 115.396\n",
            "Train Epoch 4 [Batch 100/469]\tLoss: 110.299\n",
            "Train Epoch 4 [Batch 200/469]\tLoss: 114.632\n",
            "Train Epoch 4 [Batch 300/469]\tLoss: 110.871\n",
            "Train Epoch 4 [Batch 400/469]\tLoss: 110.573\n",
            "=====> Epoch 4, Average Loss: 111.915\n",
            "=====> Average Test Loss: 110.019\n",
            "Train Epoch 5 [Batch 0/469]\tLoss: 112.920\n",
            "Train Epoch 5 [Batch 100/469]\tLoss: 109.336\n",
            "Train Epoch 5 [Batch 200/469]\tLoss: 116.019\n",
            "Train Epoch 5 [Batch 300/469]\tLoss: 109.988\n",
            "Train Epoch 5 [Batch 400/469]\tLoss: 107.297\n",
            "=====> Epoch 5, Average Loss: 110.130\n",
            "=====> Average Test Loss: 108.793\n",
            "Train Epoch 6 [Batch 0/469]\tLoss: 109.471\n",
            "Train Epoch 6 [Batch 100/469]\tLoss: 109.206\n",
            "Train Epoch 6 [Batch 200/469]\tLoss: 104.925\n",
            "Train Epoch 6 [Batch 300/469]\tLoss: 108.087\n",
            "Train Epoch 6 [Batch 400/469]\tLoss: 103.768\n",
            "=====> Epoch 6, Average Loss: 108.956\n",
            "=====> Average Test Loss: 107.966\n",
            "Train Epoch 7 [Batch 0/469]\tLoss: 106.565\n",
            "Train Epoch 7 [Batch 100/469]\tLoss: 108.199\n",
            "Train Epoch 7 [Batch 200/469]\tLoss: 106.591\n",
            "Train Epoch 7 [Batch 300/469]\tLoss: 109.144\n",
            "Train Epoch 7 [Batch 400/469]\tLoss: 110.477\n",
            "=====> Epoch 7, Average Loss: 108.087\n",
            "=====> Average Test Loss: 107.206\n",
            "Train Epoch 8 [Batch 0/469]\tLoss: 107.404\n",
            "Train Epoch 8 [Batch 100/469]\tLoss: 107.418\n",
            "Train Epoch 8 [Batch 200/469]\tLoss: 104.335\n",
            "Train Epoch 8 [Batch 300/469]\tLoss: 106.018\n",
            "Train Epoch 8 [Batch 400/469]\tLoss: 105.029\n",
            "=====> Epoch 8, Average Loss: 107.448\n",
            "=====> Average Test Loss: 106.674\n",
            "Train Epoch 9 [Batch 0/469]\tLoss: 107.519\n",
            "Train Epoch 9 [Batch 100/469]\tLoss: 101.785\n",
            "Train Epoch 9 [Batch 200/469]\tLoss: 106.007\n",
            "Train Epoch 9 [Batch 300/469]\tLoss: 104.442\n",
            "Train Epoch 9 [Batch 400/469]\tLoss: 104.377\n",
            "=====> Epoch 9, Average Loss: 106.938\n",
            "=====> Average Test Loss: 106.256\n",
            "Train Epoch 10 [Batch 0/469]\tLoss: 112.242\n",
            "Train Epoch 10 [Batch 100/469]\tLoss: 105.212\n",
            "Train Epoch 10 [Batch 200/469]\tLoss: 103.555\n",
            "Train Epoch 10 [Batch 300/469]\tLoss: 106.426\n",
            "Train Epoch 10 [Batch 400/469]\tLoss: 106.313\n",
            "=====> Epoch 10, Average Loss: 106.518\n",
            "=====> Average Test Loss: 105.814\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}